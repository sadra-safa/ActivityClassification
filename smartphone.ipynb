{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval = np.loadtxt('Datasets/UCI HAR Dataset/train/X_train.txt')\n",
    "y_trainval = np.loadtxt('Datasets/UCI HAR Dataset/train/y_train.txt')\n",
    "\n",
    "X_test = np.loadtxt('Datasets/UCI HAR Dataset/test/X_test.txt')\n",
    "y_test = np.loadtxt('Datasets/UCI HAR Dataset/test/y_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train, Val, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) #Setting seed for having reproducing results and having the same train, val, test splits\n",
    "\n",
    "trainval_subjects = np.loadtxt('Datasets/UCI HAR Dataset/train/subject_train.txt')\n",
    "trainval_subject_ids = np.unique(trainval_subjects)\n",
    "\n",
    "np.random.shuffle(trainval_subject_ids)    # shuffle subjects for splitting into train and validation sets\n",
    "train_subjects = trainval_subject_ids[:16] # 16 subjects are for training split\n",
    "val_subjects = trainval_subject_ids[16:]   #  5 subjects are for validation split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = np.zeros(trainval_subjects.shape[0], dtype=np.bool)\n",
    "for train_subject in train_subjects:\n",
    "    train_masks |= (trainval_subjects == train_subject)\n",
    "    \n",
    "val_masks = np.zeros(trainval_subjects.shape[0], dtype=np.bool)\n",
    "for val_subject in val_subjects:\n",
    "    val_masks |= (trainval_subjects == val_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_trainval[train_masks], y_trainval[train_masks]\n",
    "X_val, y_val = X_trainval[val_masks], y_trainval[val_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5609, 561), (5609,), (1743, 561), (1743,), (2947, 561), (2947,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((561,), (561,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean =  X_train.mean(0)\n",
    "std = X_train.std(0)\n",
    "mean.shape, std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-7 # adding a very small value to std to avoid dividing by zero \n",
    "X_train_normalised = (X_train - mean) / (std + eps)\n",
    "X_val_normalised = (X_val - mean) / (std + eps)\n",
    "X_test_normalised = (X_test - mean) / (std + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:   0.0, train_acc: 0.991, val_acc: 0.923\n",
      "alpha:   0.1, train_acc: 0.991, val_acc: 0.929\n",
      "alpha:   1.0, train_acc: 0.991, val_acc: 0.927\n",
      "alpha:   3.0, train_acc: 0.992, val_acc: 0.924\n",
      "alpha:  10.0, train_acc: 0.991, val_acc: 0.921\n",
      "best alpha: 0.1\n",
      "train  acc: 0.991\n",
      "val    acc: 0.929\n",
      "test   acc: 0.938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "max_val_acc = -1\n",
    "best_alpha = None\n",
    "ridge = None\n",
    "for alpha in [0, 0.1, 1, 3, 10]:\n",
    "    classifier = RidgeClassifier(alpha=alpha)\n",
    "    classifier.fit(X_train_normalised, y_train)\n",
    "    train_acc = classifier.score(X_train_normalised, y_train)\n",
    "    val_acc = classifier.score(X_val_normalised, y_val)\n",
    "    print('alpha: {:5.1f}, train_acc: {:.3f}, val_acc: {:.3f}'.format(alpha, train_acc, val_acc))\n",
    "    if val_acc > max_val_acc:\n",
    "        best_alpha = alpha\n",
    "        max_val_acc = val_acc\n",
    "        ridge = classifier\n",
    "print('best alpha: {}'.format(best_alpha))\n",
    "print('train  acc: {:.3f}'.format(ridge.score(X_train_normalised, y_train)))\n",
    "print('val    acc: {:.3f}'.format(ridge.score(X_val_normalised, y_val)))\n",
    "print('test   acc: {:.3f}'.format(ridge.score(X_test_normalised, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:   0.1, train_acc: 0.997, val_acc: 0.935\n",
      "C:   1.0, train_acc: 1.000, val_acc: 0.920\n",
      "C:   3.0, train_acc: 1.000, val_acc: 0.915\n",
      "C:  10.0, train_acc: 1.000, val_acc: 0.909\n",
      "best C: 0.1\n",
      "train  acc: 0.997\n",
      "val    acc: 0.935\n",
      "test   acc: 0.941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "max_val_acc = -1\n",
    "best_C = None\n",
    "linear_svc = None\n",
    "for c in [0.1, 1, 3, 10]:\n",
    "    classifier = LinearSVC(C=c, dual=False)\n",
    "    classifier.fit(X_train_normalised, y_train)\n",
    "    train_acc = classifier.score(X_train_normalised, y_train)\n",
    "    val_acc = classifier.score(X_val_normalised, y_val)\n",
    "    print('C: {:5.1f}, train_acc: {:.3f}, val_acc: {:.3f}'.format(c, train_acc, val_acc))\n",
    "    if val_acc > max_val_acc:\n",
    "        best_C = c\n",
    "        max_val_acc = val_acc\n",
    "        linear_svc = classifier\n",
    "print('best C: {}'.format(best_C))\n",
    "print('train  acc: {:.3f}'.format(linear_svc.score(X_train_normalised, y_train)))\n",
    "print('val    acc: {:.3f}'.format(linear_svc.score(X_val_normalised, y_val)))\n",
    "print('test   acc: {:.3f}'.format(linear_svc.score(X_test_normalised, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: poly, train_acc: 0.983, val_acc: 0.933\n",
      "Kernel: rbf, train_acc: 0.990, val_acc: 0.923\n",
      "Kernel: sigmoid, train_acc: 0.838, val_acc: 0.788\n",
      "best kernel: poly\n",
      "train  acc: 0.983\n",
      "val    acc: 0.933\n",
      "test   acc: 0.909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "max_val_acc = -1\n",
    "best_kernel = None\n",
    "svc = None\n",
    "for kernel in ['poly', 'rbf', 'sigmoid']:\n",
    "    classifier = SVC(kernel=kernel)\n",
    "    classifier.fit(X_train_normalised, y_train)\n",
    "    train_acc = classifier.score(X_train_normalised, y_train)\n",
    "    val_acc = classifier.score(X_val_normalised, y_val)\n",
    "    print('Kernel: {}, train_acc: {:.3f}, val_acc: {:.3f}'.format(kernel, train_acc, val_acc))\n",
    "    if val_acc > max_val_acc:\n",
    "        best_kernel = kernel\n",
    "        max_val_acc = val_acc\n",
    "        svc = classifier\n",
    "print('best kernel: {}'.format(best_kernel))\n",
    "print('train  acc: {:.3f}'.format(svc.score(X_train_normalised, y_train)))\n",
    "print('val    acc: {:.3f}'.format(svc.score(X_val_normalised, y_val)))\n",
    "print('test   acc: {:.3f}'.format(svc.score(X_test_normalised, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  acc: 0.541\n",
      "val    acc: 0.558\n",
      "test   acc: 0.531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(X_train_normalised, y_train)\n",
    "print('train  acc: {:.3f}'.format(adaboost.score(X_train_normalised, y_train)))\n",
    "print('val    acc: {:.3f}'.format(adaboost.score(X_val_normalised, y_val)))\n",
    "print('test   acc: {:.3f}'.format(adaboost.score(X_test_normalised, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  acc: 1.000\n",
      "val    acc: 0.871\n",
      "test   acc: 0.898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train_normalised, y_train)\n",
    "print('train  acc: {:.3f}'.format(random_forest.score(X_train_normalised, y_train)))\n",
    "print('val    acc: {:.3f}'.format(random_forest.score(X_val_normalised, y_val)))\n",
    "print('test   acc: {:.3f}'.format(random_forest.score(X_test_normalised, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 1, train_acc: 1.000, val_acc: 0.832\n",
      "K: 3, train_acc: 0.992, val_acc: 0.851\n",
      "K: 5, train_acc: 0.984, val_acc: 0.857\n",
      "K: 7, train_acc: 0.979, val_acc: 0.858\n",
      "K: 9, train_acc: 0.976, val_acc: 0.864\n",
      "best K: 9\n",
      "train  acc: 0.976\n",
      "val    acc: 0.864\n",
      "test   acc: 0.870\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "max_val_acc = -1\n",
    "best_k = None\n",
    "knn = None\n",
    "for k in [1, 3, 5, 7, 9]:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    classifier.fit(X_train_normalised, y_train)\n",
    "    train_acc = classifier.score(X_train_normalised, y_train)\n",
    "    val_acc = classifier.score(X_val_normalised, y_val)\n",
    "    print('K: {}, train_acc: {:.3f}, val_acc: {:.3f}'.format(k, train_acc, val_acc))\n",
    "    if val_acc > max_val_acc:\n",
    "        best_k = k\n",
    "        max_val_acc = val_acc\n",
    "        knn = classifier\n",
    "print('best K: {}'.format(best_k))\n",
    "print('train  acc: {:.3f}'.format(knn.score(X_train_normalised, y_train)))\n",
    "print('val    acc: {:.3f}'.format(knn.score(X_val_normalised, y_val)))\n",
    "print('test   acc: {:.3f}'.format(knn.score(X_test_normalised, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: gini, train_acc: 1.000, val_acc: 0.806\n",
      "Criterion: entropy, train_acc: 1.000, val_acc: 0.795\n",
      "Criterion: log_loss, train_acc: 1.000, val_acc: 0.788\n",
      "best Criterion: gini\n",
      "train  acc: 1.000\n",
      "val    acc: 0.806\n",
      "test   acc: 0.793\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_val_acc = -1\n",
    "best_criterion = None\n",
    "decisiontree = None\n",
    "for criterion in ['gini', 'entropy', 'log_loss']:\n",
    "    classifier = DecisionTreeClassifier(criterion=criterion)\n",
    "    classifier.fit(X_train_normalised, y_train)\n",
    "    train_acc = classifier.score(X_train_normalised, y_train)\n",
    "    val_acc = classifier.score(X_val_normalised, y_val)\n",
    "    print('Criterion: {}, train_acc: {:.3f}, val_acc: {:.3f}'.format(criterion, train_acc, val_acc))\n",
    "    if val_acc > max_val_acc:\n",
    "        best_criterion = criterion\n",
    "        max_val_acc = val_acc\n",
    "        decisiontree = classifier\n",
    "print('best Criterion: {}'.format(best_criterion))\n",
    "print('train  acc: {:.3f}'.format(decisiontree.score(X_train_normalised, y_train)))\n",
    "print('val    acc: {:.3f}'.format(decisiontree.score(X_val_normalised, y_val)))\n",
    "print('test   acc: {:.3f}'.format(decisiontree.score(X_test_normalised, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neurons, activation):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in  range(len(neurons) - 1):\n",
    "            layers.append(nn.Linear(neurons[i], neurons[i+1]))\n",
    "            if activation == 'ReLU':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'LeakyReLU':\n",
    "                layers.append(nn.LeakyReLU())\n",
    "            elif activation == 'Sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif activation == 'Tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            else:\n",
    "                raise ValueError('Activation must be one of ReLU, LeakyReLU, sigmoid, or Tanh')\n",
    "        self.mlp = nn.Sequential(*layers[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying [561, 1024, 6] with ReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with ReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with ReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with ReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with ReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 6] with ReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with LeakyReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 6] with LeakyReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with Sigmoid Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 6] with Sigmoid Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 6] with Tanh Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with Tanh Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 6] with Tanh Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with Tanh Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 6] with Tanh Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 6] with Tanh Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with ReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with ReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with ReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with ReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with ReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with ReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Tanh Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Tanh Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Tanh Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Tanh Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Tanh Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 128, 6] with Tanh Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with ReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with ReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with ReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with ReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with ReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with ReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Tanh Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Tanh Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Tanh Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Tanh Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Tanh Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 512, 128, 6] with Tanh Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with ReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with ReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with ReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with ReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with ReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with ReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with LeakyReLU Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with LeakyReLU Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Sigmoid Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Sigmoid Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Tanh Activation Layer and SGD Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Tanh Activation Layer and Adam Optimiser with 0.001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Tanh Activation Layer and SGD Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Tanh Activation Layer and Adam Optimiser with 0.0001 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Tanh Activation Layer and SGD Optimiser with 1e-05 Learning Rate\n",
      "Trying [561, 1024, 1024, 128, 6] with Tanh Activation Layer and Adam Optimiser with 1e-05 Learning Rate\n",
      "-------------------------------\n",
      "Best Validation: 0.9311531782150269\n",
      "Achieved with [561, 1024, 128, 6] Architecture and LeakyReLU Activation Layer and Adam Optimiser with 0.001 Learning Rate\n"
     ]
    }
   ],
   "source": [
    "n_classes = 6\n",
    "architectures = [[X_train_normalised.shape[1], 1024, n_classes],\n",
    "                 [X_train_normalised.shape[1], 1024, 128, n_classes],\n",
    "                 [X_train_normalised.shape[1], 1024, 512, 128, n_classes],\n",
    "                 [X_train_normalised.shape[1], 1024, 1024, 128, n_classes]]\n",
    "activations = ['ReLU', 'LeakyReLU', 'Sigmoid', 'Tanh']\n",
    "lrs = [0.001, 0.0001, 0.00001]\n",
    "optimisers = ['SGD', 'Adam']\n",
    "\n",
    "best_val = -1\n",
    "best_lr = -1\n",
    "best_architecture = None\n",
    "best_activation = None\n",
    "best_opt = None\n",
    "best_mlp = None\n",
    "\n",
    "\n",
    "for arch in architectures:\n",
    "    for activation in activations:\n",
    "        for lr in lrs:\n",
    "            for opt in optimisers:\n",
    "                print('Trying {} with {} Activation Layer and {} Optimiser with {} Learning Rate'.format(arch, activation, opt, lr))\n",
    "                mlp = MLP(arch, activation).cuda()\n",
    "                mlp.apply(init_weights)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                if opt == 'SGD':\n",
    "                    optimiser = optim.SGD(mlp.parameters(), lr=lr)\n",
    "                else:\n",
    "                    optimiser = optim.Adam(mlp.parameters(), lr=lr)\n",
    "                num_epochs = 100\n",
    "                for epoch in range(num_epochs):\n",
    "                    mlp.train()\n",
    "                    optimiser.zero_grad()\n",
    "                    output = mlp(torch.tensor(X_train_normalised).float().cuda())\n",
    "                    loss = criterion(output, torch.tensor(y_train - 1).long().cuda())\n",
    "                    loss.backward()\n",
    "                    optimiser.step()\n",
    "                    mlp.eval()\n",
    "                    with torch.no_grad():\n",
    "                        output = mlp(torch.tensor(X_val_normalised).float().cuda())\n",
    "                        pred = torch.argmax(output, 1)\n",
    "                        correct = (pred == torch.tensor(y_val - 1).long().cuda()).sum()\n",
    "                        val = (correct / output.shape[0]).item()\n",
    "                        if val > best_val:\n",
    "                            best_val = val\n",
    "                            best_lr = lr\n",
    "                            best_architecture = arch\n",
    "                            best_activation = activation\n",
    "                            best_opt = opt\n",
    "                            best_mlp = copy.deepcopy(mlp)\n",
    "\n",
    "print('-------------------------------')                    \n",
    "print('Best Validation: {}'.format(best_val))\n",
    "print('Achieved with {} Architecture and {} Activation Layer and {} Optimiser with {} Learning Rate'.format(best_architecture, best_activation, best_opt, best_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.9980388879776001\n",
      "val   accuracy:  0.9311531782150269\n",
      "test  accuracy:  0.9290804266929626\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    best_mlp.eval()\n",
    "    \n",
    "    output = best_mlp(torch.tensor(X_train_normalised).float().cuda())\n",
    "    pred = torch.argmax(output, 1)\n",
    "    correct = (pred == torch.tensor(y_train - 1).long().cuda()).sum()\n",
    "    train_acc = (correct / output.shape[0]).item()\n",
    "    print('train accuracy: ', train_acc)\n",
    "    \n",
    "    output = best_mlp(torch.tensor(X_val_normalised).float().cuda())\n",
    "    pred = torch.argmax(output, 1)\n",
    "    correct = (pred == torch.tensor(y_val - 1).long().cuda()).sum()\n",
    "    val_acc = (correct / output.shape[0]).item()\n",
    "    print('val   accuracy: ', val_acc)\n",
    "    \n",
    "    output = best_mlp(torch.tensor(X_test_normalised).float().cuda())\n",
    "    pred = torch.argmax(output, 1)\n",
    "    correct = (pred == torch.tensor(y_test - 1).long().cuda()).sum()\n",
    "    test_acc = (correct / output.shape[0]).item()\n",
    "    print('test  accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
